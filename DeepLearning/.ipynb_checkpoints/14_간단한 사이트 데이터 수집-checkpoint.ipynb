{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수집할 페이지의 주소\n",
    "- https://pjt3591oo.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수집된 데이터는 csv파일로 저장\n",
    "- user-agent 확인할 수 있는 사이트 : http://m.avalon.co.kr/check.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time # 딜레이를 주기 위해 \n",
    "import os\n",
    "from IPython.display import clear_output # 변경된 결과만 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요청하는 함수\n",
    "def getSource(site):\n",
    "    # 헤더 정보\n",
    "    header_info = {\n",
    "        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36'\n",
    "    }\n",
    "    # 요청한다\n",
    "    response = requests.get(site, headers=header_info)\n",
    "    # print(response.text)\n",
    "    \n",
    "    # bs4 객체 생성\n",
    "    soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 페이지의 데이터를 수집해 저장하는 함수\n",
    "def getData(soup):\n",
    "    # print(soup)\n",
    "    # 데이터가 있는 영역 전체를 가져온다\n",
    "    a1 = soup.select_one('body > main > div > div')\n",
    "    # print(a1)\n",
    "    # 데이터가 있는 p 태그를 가져온다\n",
    "    # a2 = a1.select('div.p')\n",
    "    a2 = soup.select('body > main > div > div > div')[:-1]\n",
    "    # print(a2)\n",
    "    \n",
    "    # 데이터를 담은 딕셔너리\n",
    "    data_dict = {\n",
    "        '큰제목' : [],\n",
    "        '작은제목' : [],\n",
    "        '날짜' : [],\n",
    "        '작성자' : []\n",
    "    }\n",
    "    \n",
    "    # div 태그의 수만큼 반복한다\n",
    "    for div in a2:\n",
    "        # 큰 제목을 가져온다\n",
    "        a3 = div.select_one('h3 > a')\n",
    "        data1 = a3.text.strip()\n",
    "        \n",
    "        # 작은 제목을 가져온다\n",
    "        a4 = div.select_one('h4')\n",
    "        data2 = a4.text.strip()\n",
    "        # print(data2)\n",
    "        # 만약 길이가 0인 문자열이 추출되었다면 결측치로 변경한다\n",
    "        if len(data2) == 0:\n",
    "            data2 = np.nan\n",
    "         \n",
    "        # 날짜, 이름을 가져온다.\n",
    "        a5 = div.select_one('p > span')\n",
    "        data3 = a5.text.strip()\n",
    "        # print(data3)\n",
    "        data4 = data3.split('|')\n",
    "        data5 = data4[0].strip()\n",
    "        data6 = data4[1].strip()\n",
    "        # print(data5)\n",
    "        # print(data6)\n",
    "        \n",
    "        # print(data1)\n",
    "        # print(data2)\n",
    "        # print(data5)\n",
    "        # print(data6)\n",
    "        \n",
    "        # 딕셔너리에 데이터를 담는다\n",
    "        data_dict['큰제목'].append(data1)\n",
    "        data_dict['작은제목'].append(data2)\n",
    "        data_dict['날짜'].append(data5)\n",
    "        data_dict['작성자'].append(data6)\n",
    "    \n",
    "    # 데이터 프레임 생성\n",
    "    df1 = pd.DataFrame(data_dict)\n",
    "    # display(df1)\n",
    "    \n",
    "    if os.path.exists('data1.csv') == False:\n",
    "        # 파일이 없을 경우\n",
    "        df1.to_csv('data1.csv', encoding='utf-8-sig', index=False) # encoding='utf-8-sig' : 엑셀로 열 때 한글이 안깨지도록\n",
    "    else:\n",
    "        df1.to_csv('data1.csv', encoding='utf-8-sig', index=False, header=False, mode='a') # mode='a' 기존의 것에 붙여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 페이지 존재 여부 확인하는 함수\n",
    "def getNextPage(soup):\n",
    "    # next 버튼 태그를 가져온다\n",
    "    a1 = soup.select_one('body > main > div > div > div.pagination > ul > li:nth-child(6) > a')\n",
    "    # print(a1)\n",
    "    # 만약 가져온 것이 있다면\n",
    "    if a1 != None:\n",
    "        # a태그의 href 속성 값을 가져온다\n",
    "        href = a1.attrs['href']\n",
    "        # print(gref)\n",
    "        return href\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수집완료\n"
     ]
    }
   ],
   "source": [
    "site = 'https://pjt3591oo.github.io'\n",
    "page = ''\n",
    "\n",
    "while True:\n",
    "    # 딜레이\n",
    "    time.sleep(1)\n",
    "    # 기존 출력된 것 청소하기\n",
    "    clear_output(wait=True)\n",
    "    # 페이지 요청\n",
    "    soup = getSource(site + page)\n",
    "    # 데이터 수집\n",
    "    getData(soup)\n",
    "    # 다음 페이지 존재 확인\n",
    "    page = getNextPage(soup)\n",
    "    # 다음 페이지가 없다면 중단한다\n",
    "    if page == None:\n",
    "        print('수집완료')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
