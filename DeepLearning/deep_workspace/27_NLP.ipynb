{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경고 메시지가 안나오게..\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 기본\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import missingno\n",
    "\n",
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 교차 검증 함수\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# 학습 데이터와 검증데이터로 나누는 함수\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 전처리\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 하이퍼 파라미터 튜닝\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 평가함수\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 머신러닝 알고리즘 - 분류\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 머신러닝 알고리즘 - 회귀\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 군집\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "# 차원축소\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 딥러닝\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# 다중분류를 위한 핫-윈 인코더\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 저장된 딥러닝 모델을 복구하는 함수\n",
    "from keras.models import load_model\n",
    "\n",
    "# epoch마다 모델을 저장하는 함수\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# 더이상 성능 향상이 이루어지지 않는다면 조기 중단시킬 수 있는 함수\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# 문장을 잘라준다\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# 저장\n",
    "import pickle\n",
    "\n",
    "# 시간 모듈\n",
    "import time\n",
    "\n",
    "# 그래프 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['figure.figsize'] = 20, 10\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 문자열\n",
    "text = '해보지 않으면 해낼 수 없다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : 해보지 않으면 해낼 수 없다\n",
      "토큰화 : ['해보지', '않으면', '해낼', '수', '없다']\n"
     ]
    }
   ],
   "source": [
    "# 해당 텍스트를 토큰화 한다\n",
    "result = text_to_word_sequence(text)\n",
    "print(f'원본 : {text}')\n",
    "print(f'토큰화 : {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 빈도수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    '먼저 텍스트의 각 단어를 나누어 토큰화 합니다',\n",
    "    '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다',\n",
    "    '토큰화 한 결과는 딥러닝에서 사용할 수 있습니다'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수를 통해 전처리를 한다\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('먼저', 1),\n",
       "             ('텍스트의', 2),\n",
       "             ('각', 1),\n",
       "             ('단어를', 1),\n",
       "             ('나누어', 1),\n",
       "             ('토큰화', 3),\n",
       "             ('합니다', 1),\n",
       "             ('단어로', 1),\n",
       "             ('해야', 1),\n",
       "             ('딥러닝에서', 2),\n",
       "             ('인식됩니다', 1),\n",
       "             ('한', 1),\n",
       "             ('결과는', 1),\n",
       "             ('사용할', 1),\n",
       "             ('수', 1),\n",
       "             ('있습니다', 1)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 빈도수\n",
    "token.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 문장의 개수\n",
    "token.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'합니다': 1,\n",
       "             '먼저': 1,\n",
       "             '토큰화': 3,\n",
       "             '나누어': 1,\n",
       "             '단어를': 1,\n",
       "             '텍스트의': 2,\n",
       "             '각': 1,\n",
       "             '단어로': 1,\n",
       "             '인식됩니다': 1,\n",
       "             '해야': 1,\n",
       "             '딥러닝에서': 2,\n",
       "             '결과는': 1,\n",
       "             '한': 1,\n",
       "             '사용할': 1,\n",
       "             '있습니다': 1,\n",
       "             '수': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어가 몇 개의 문장에 포함되어 있는가\n",
    "token.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'토큰화': 1,\n",
       " '텍스트의': 2,\n",
       " '딥러닝에서': 3,\n",
       " '먼저': 4,\n",
       " '각': 5,\n",
       " '단어를': 6,\n",
       " '나누어': 7,\n",
       " '합니다': 8,\n",
       " '단어로': 9,\n",
       " '해야': 10,\n",
       " '인식됩니다': 11,\n",
       " '한': 12,\n",
       " '결과는': 13,\n",
       " '사용할': 14,\n",
       " '수': 15,\n",
       " '있습니다': 16}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어에 부여된 인덱스\n",
    "token.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가글\n",
    "docs = [\n",
    "    '너무 재밌네요',\n",
    "    '최고예요',\n",
    "    '참 잘 만든 영화에요',\n",
    "    '추천하고 싶은 영화입니다',\n",
    "    '한번 더 보고 싶네요',\n",
    "    '글쎄요',\n",
    "    '별로에요',\n",
    "    '생각보다 지루하네요',\n",
    "    '연기가 어색해요',\n",
    "    '재미없어요'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 데이터 \n",
    "classes = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'너무': 1,\n",
       " '재밌네요': 2,\n",
       " '최고예요': 3,\n",
       " '참': 4,\n",
       " '잘': 5,\n",
       " '만든': 6,\n",
       " '영화에요': 7,\n",
       " '추천하고': 8,\n",
       " '싶은': 9,\n",
       " '영화입니다': 10,\n",
       " '한번': 11,\n",
       " '더': 12,\n",
       " '보고': 13,\n",
       " '싶네요': 14,\n",
       " '글쎄요': 15,\n",
       " '별로에요': 16,\n",
       " '생각보다': 17,\n",
       " '지루하네요': 18,\n",
       " '연기가': 19,\n",
       " '어색해요': 20,\n",
       " '재미없어요': 21}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 인덱스\n",
    "token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2],\n",
       " [3],\n",
       " [4, 5, 6, 7],\n",
       " [8, 9, 10],\n",
       " [11, 12, 13, 14],\n",
       " [15],\n",
       " [16],\n",
       " [17, 18],\n",
       " [19, 20],\n",
       " [21]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 문장을 토큰화 시킨 데이터를 단어 인덱스로 변환한다\n",
    "x = token.texts_to_sequences(docs)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  1,  2],\n",
       "       [ 0,  0,  0,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 0,  8,  9, 10],\n",
       "       [11, 12, 13, 14],\n",
       "       [ 0,  0,  0, 15],\n",
       "       [ 0,  0,  0, 16],\n",
       "       [ 0,  0, 17, 18],\n",
       "       [ 0,  0, 19, 20],\n",
       "       [ 0,  0,  0, 21]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 리스트의 데이터의 개수를 최대 개수로 통일한다 => padding\n",
    "padded_x = pad_sequences(x, 4)\n",
    "padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  단어의 수를 파악한다\n",
    "# 0인덱스가 붙기 때문에 1을 더해줌\n",
    "word_size = len(token.word_index) + 1\n",
    "word_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습모델을 구성한다\n",
    "model = Sequential()\n",
    "\n",
    "# 임베딩(Embedding) : 많은 단어의 수를 줄인다\n",
    "# 첫 번째 : 전체 단어의 수\n",
    "# 두 번째 : 줄어든 사이즈 => 작으면 속도가 빠르지만 정확도가 낮고 / 크면 속도가 느리지만 정확도가 높다\n",
    "# 세 번쨰 : 입력층의 노드의 개수\n",
    "\n",
    "model.add(Embedding(word_size, 8, input_length=4))\n",
    "\n",
    "# 1차원으로 변경 - 활성함수 들어가기 전에 1차원으로 바껴야 함\n",
    "model.add(Flatten())\n",
    "\n",
    "# 출력\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 950ms/step - loss: 0.6906 - accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6887 - accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6868 - accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6849 - accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6830 - accuracy: 0.7000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6811 - accuracy: 0.8000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6792 - accuracy: 0.8000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6773 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6754 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6735 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6716 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6697 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6678 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6659 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6640 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6621 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6601 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6582 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6562 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6543 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1373b643588>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "model.fit(padded_x, classes, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6523 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(padded_x, classes)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
